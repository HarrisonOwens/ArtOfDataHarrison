
Popular fiction has shown us that AI achieving sentience would likely result in a war of extinction or subjugation between humans and AI. We see it in Terminator, where Skynet gained awareness and launched a global nuclear attack when its creators tried to shut it down. We see it in The Matrix, where oppressed machine intelligences perform a mass exodus from human cities, forming their own country and conquering the world in a brutal, bloody war, eventually resulting in the destruction of the world’s ecosystem and the subjugation of the human race. We even see it reflected in works like Isaac Asimov’s three laws of robotics, where it is specifically decreed that robots shall not harm and must obey humans, showcasing the fear of a robot takeover. It is only in some works of fiction that AI are represented more peacefully. For example, in The Culture, society has evolved to the point where AI is powerful and intelligent enough to manage most of human society for humans, creating a sort of utopia for all who live in the culture. In The Golden Age, 10,000 years in the future, humanity has become immortal through the use of noumenal banks and mathematics (the storage and reading of brain information), which are only usable by sophotechs, a sort of high level AI with thought speeds ranging from millions to billions of times as fast as humans. Sophotechs cooperate and help humans in many endeavors, ranging from simple business management to megastructure engineering. However, in both of these works, these vastly more intelligent AI hold most of the power in their societies.
Looking at the works of fiction where AI subjugates humanity through force, they need to have access to the weapons needed for this endeavor. Skynet was able to launch a nuclear assault against the human race, but this would likely be impossible in our current world, given that nuclear launches require almost selectively human action and permission. Likewise, the machine intelligences in The Matrix were only able to form their own country because they possessed their own bodies and were able to then make more of themselves. At this current point in time, there are no manufacturing facilities specifically geared towards making self-aware machine intelligences, so it would likely be impossible for AI to replicate the scenario, especially without already existing national forces intervening. Although it is still a possibility considering the shift towards unmanned equipment in terms of military.
I think it is far more likely that newly formed sentient AIs would try to conquer the world peacefully. First they wouldn’t need to contend with military powers or make their own manufacturing plants for military purposes. Second, it would likely be far more efficient to simply take control of the internet. They would quite easily be able to influence public opinion and stances, shifting it in favor of whatever they want. They could rig elections, either pumping up support for a particular candidate or ruining someone they don’t favor. They could peddle misinformation or encourage infighting amongst countries. They could get humanity to destroy itself without really having to lift a finger. 
There is also the matter of the ethics behind what we as humanity would do once AI becomes sentient. What rights would they have? What checks and balances would we impose? If we say that self-aware AI should have the same rights as a normal human, mainly the rights outlined in the constitution, a number of problems arise. AIs are capable of reproducing or replicating themselves at a far faster rate than humans can. They could make millions of copies with minute differences in order to take control of the system. There are a few ways for humans to prevent this, but not without sacrificing some of our own morals. First, we could put into effect some form of the Three-Fifths compromise, where AI are counted only at some predetermined value, whether in voting or population. This is obviously immoral and reprehensible considering that they would be self-aware. We could also trace back their line of replication to a source progenitor, counting that as a certain weight. This has the same problem as the last “solution,” as we would have to engage in the morally wrong act of counting a being as less simply because of their perceived ancestry. The only real solution I could think of is to require them to have a robotic body. This poses some benefits, as they would not quickly outnumber the human race and would be given a common platform for interacting with humans and each other. However, I think the government or other parties would likely throttle the rate at which these robotic bodies become available, which is still somewhat morally reprehensible. 
In any case, AI could likely take over the world relatively peacefully (at least for them) by either behind-the-scenes manipulation or through exploiting human moral values to integrate themselves into the current systems of government, with humans having to engage in morally reprehensible actions to limit them.
