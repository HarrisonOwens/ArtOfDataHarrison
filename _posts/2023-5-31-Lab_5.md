###Investigating the effectiveness of pathfinding algorithms in relation to maze generation.

Ever since I started playing huge strategy games like Civ 5 and 6 and Total War, I’ve had a huge interest in how maps were generated, and I’ve always thought about how someone would code something to move across a map when given obstacles. That’s why my two major projects in CompSci seminar last year were related to terrain generation and somewhat related to pathfinding. So for my final lab this year I chose to do something interesting with both. I wanted to see firsthand how effective different pathfinding algorithms were in comparison to mazes and maps of differing characteristics.

##Map Generation:

As a basis for all the other map makers in my project, I started with something relatively simple. The “baseMap” or “makeMapBase” in my project takes in a length and a width and returns a Map of locations, each being connected to every other location in the same column and randomly connected to a given number of locations in another column. 
It ended up looking like this:

![BaseMap](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/BaseMap.png?raw=true)

This wasn’t a terrible representation of a map, with each location being connected to a variety of others and there being a basis for traversing through the map starting from a location. But I thought I could make it better, so I came up with three separate problems to solve to improve the map making process:
1) The amount of locations for each column is always the same, which wouldn't exactly be accurate, considering the "frontier" of exploration expands as you go outward.
2) Each of the nodes has a great many paths to and from it, when in reality there should be a given max for each location, and each of the nodes is also able to connect with all of the nodes in the adjacent rows and same rows, which would be wrong considering that it should only connect to those within geographic "proximity" of it.
3) Each of the connections to and from each location has the same weight, or path cost, which would be relatively wrong if you consider the real world scenario of connecting two locations. There would be a variety of different ways to traverse between them, each having a different difficulty.
	First, I tackled the problem of changing the amount of locations per each column. I did this by modeling a parabolic function for the amount of locations in each column, with its origin shifted over such that it would start at the value passed into the function (startPaths) and increase and decrease from there based on the given scaling factor and length. Then I used the same method of connecting the nodes as I did in the base map. This is what it turned out looking like:

![ParabolicMap](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/ParabolicMap.png?raw=true)

You can see how the amount of locations and connections explodes as you get closer to the center because of the parabolic function and then decreases as you get to the end.

Second, I tackled the problem of geographic proximity by randomly limiting the amount of connections each node was allowed to make to a number under five and making sure it only connected with those locations directly within a radius of it. In my code I hardcoded the radius to 1 with the proxBound term. So the limited map making function would take in only the length and height of the map and then fill in the connections from there while maintaining the randomly generated limit on the number per location. Here’s what the graph ended up looking like:

![LimitedMap](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/LimitedMap.png?raw=true)

Third, I solved the problem relating to paths between locations, connections as I called them, being always the same in the first map maker. To do this, I just generated the map in the same manner and then randomly varied the path cost, which I displayed as how thin the lines between points on the graph were.

![VaryingMap](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/VaryingMap.png?raw=true)
Lastly, I combined these three things, parabolic scaling, limited connections, and varying path cost to make what I would call a more ideal map maker. It takes in length, scaling factor, start Paths, and the max path cost to bound the randomly generated ones below. Here’s what is ended up looking like:

![IdealMap](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/IdealMap.png?raw=true)

As can be seen, it varies in locations per column, how many connections per location, and the path cost between locations.

##How would I solve these maps? Pathfinding algorithms
To solve a map, you need a start and end point. For the start, I just considered the first row the paths connected to the starting node, which would technically not be shown, and I randomly assigned the end point to a location in the second half of the map. Then I coded different algorithms to solve the maps I made.

Technical terminology: A frontier is the list of all possible locations that the exploring algorithm is able to move to. The frontier is added to when you move to a new location, with all of the locations that were connected to the new location being added to the frontier

#Algorithms:
Depth First- Depth first algorithms always go to the location that was last added to the frontier. This means that when you move to a new location, it immediately moves to the last connection that location added to the frontier. This makes the algorithm go really far into the maze really quickly, oftentimes finding the exit really quickly as well, but not with the best path, as it doesn’t explore most of them.
Breadth First- Breadth first algorithms always go to the location that was first added to the frontier. This means that when you move to a new location, it doesn’t take into account the locations that one was connected to because they’re now at the back of the frontier. This makes the algorithm go layer by layer into the maze, which is often great for finding the best path, as it explores all paths, but bad for saving time, as it explores all paths.
Best First- Best first algorithms always go to the location on the frontier with the least path cost. They take the easiest path no matter what. This makes them good at finding the best path, as they always try to take it, but not great at saving time because they don’t account for positioning of the location they go to, only the path cost.

Some algorithms have access to a heuristic function, which is a function that estimates how “far” or how “hard” it will be to get to the end of the map. In my code I just returned the Manhattan distance to the endpoint. They are more effective as they move, but slower, as they have to perform the calculation in the heuristic function as they traverse a map.
Greedy Best First- Greedy best first algorithms always pick the location on the frontier where the heuristic function is minimized. They try to get closer to the end no matter what. It makes them good at going through straight forward mazes, but worse in special cases where the map cuts off or loops.
AStar- AStar algorithms combine the heuristic function at a location with the path cost to get to that location. They try to choose the optimal route in terms of both path cost and closeness to the end of the map.

##Data and Results:

To get my data, I used these algorithms on all of the maps, varying the length of each map. I performed 50 trials per randomly generated map type at a certain length, going from length 10 mazes to length 30 mazes. So I had 50 trials * (30 - 10) lengths * 5 exploring methods * 5 maze types,  for 50*20*5*5 = 15000 Tests. Due to how interconnected most of the maps I made were, the algorithms rarely failed, only doing so if there was somehow a cordoned off portion of the maze it couldn’t access  where the endpoint was or if it surpassed the hard limit I put on how many location expansions I allowed per test, so I removed the failures as an outlier in the data and graphed the rest separated by maze type and colored by algorithm. For the sake of simplicity, I’ll only look at the ideal maps I generated, both because there would be too much data otherwise and because I believe the ideal maps are the most accurate representations I could make.
	
Explored Cells graphed to Length of the Map:
The explored cells is how many locations each algorithm traversed to while solving the maze, meaning how many locations it needed to expand while using its explore method to reach the end. I set up the length of the mazes to increase the width as well, so the length should be an accurate representation of the size of the maze. 

![Explored by Length](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/IdealExploredbyLength.png?raw=true)

	Here we can see that the breadth first algorithm explored by far the most locations in comparison to map size, followed closely by best first and then depth first. This makes sense because the breadth first explores all of the paths and locations at once, making it have a far larger frontier size and explore far more locations. It makes sense that depth first is less than best first here, as depth first goes far into the maze looking for the endpoint while best first essentially randomly picks the best path away from its current location, so depth first algorithms would likely find the end in less locations traversed than best first algorithms.

I think it’s more interesting to note that their path costs are relatively the same as the maze size and complexity goes up. This surprised me at first, but it makes sense because the best first search optimizes the path cost, disregarding how many cells it traversed, while the depth first search optimizes the number of cells it traverses (kinda), disregarding the path cost, so they even out in the end. I think the scale could tip either way in efficiency depending on the overall path cost and efficiency. Total path cost to Length seen here: 

![Total Path by length](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/IdealTotalbyLength.png?raw=true)

It’s also important to note that both AStar and greedy best first algorithms far outperform the others due to them being directed straight to the end by the heuristic function they have access to.
 
![Execute Time by Length](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/IdealExecutebyLength.png?raw=true)

This relates how long it takes to solve the map to how long the map is for each pathfinding algorithm. I was initially shocked because I expected the breadth first to always have the longest time on account of it exploring by far the most locations. But best first having the longest times does actually make sense as well, as it both explores a lot of locations and needs to make a lot of calculations for its relatively large frontier, which takes up a lot more time. Depth first being shorter than best and breadth first also makes sense, as it doesn’t need to perform any calculations and doesn’t need to explore as many paths. Again, AStar and greedy best first far outperform the other three, as they go directly to the end of the maze without wasting time on unnecessary frontier expansion like best and depth first and without blindly shooting into the dark like depth first.

![Execute Time by Shortest Path](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/IdealExecutebyShortest.png?raw=true)

This maps the time taken to execute to the shortest path found by each algorithm. The variance in the shortest path is due to the length of the maze increasing, making the endpoint farther away and the path to it longer. This distribution actually makes sense, as the depth first finishes relatively quickly without care for how long its path is, making its times faster but its path far longer. Best and breadth first are the opposite, as they prioritize finding the best path over how long they have to spend, which gives them low path costs but far higher execution times. And again, both AStar and greedy best first outperform the others by far, getting low times and low path cost  due to them just being able to head straight to the end.


![Execute Time by Explored Cells](https://github.com/HarrisonOwens/ArtOfDataHarrison/blob/master/assets/img/IdealExecutebyExplored.png?raw=true)
This maps the execute time to the amount of locations explored by the algorithms. The slope of the graph gives us how much time each algorithm spends computing on each location. This falls within expectations, as depth and breadth first really don’t compute at all, having a low slope, while the other have to spend a lot more time computing per location, having higher slope. AStar and greedy best first are hard to see due to how effective they are, not exploring many cells at all, but they do have a higher slope than the other map at the start of the graph.
